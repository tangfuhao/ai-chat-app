import { OpenAI } from "openai"
import Anthropic from "@anthropic-ai/sdk"
import axios from 'axios'
import { GoogleGenAI } from "@google/genai"
import { getModelConfigByModelName, validateAndNormalizeParams } from "@/lib/model-config"

interface Message {
  role: 'user' | 'assistant' | 'system'
  content: string
}

export const maxDuration = 300

export async function POST(req: Request) {
  try {
    const { messages, apiKey, model, provider, parameters = {} } = await req.json()

    console.log('ğŸ“¥ æ”¶åˆ°è«‹æ±‚:', {
      provider,
      model,
      messageCount: messages?.length,
      parameters
    })

    // åƒæ•¸é©—è­‰
    if (!apiKey) {
      console.error('âŒ ç¼ºå°‘ API Key')
      return errorResponse(400, "è«‹æä¾›æœ‰æ•ˆçš„ API Key")
    }

    if (!model) {
      console.error('âŒ ç¼ºå°‘æ¨¡å‹åç¨±')
      return errorResponse(400, "è«‹æä¾›æ¨¡å‹åç¨±")
    }

    // ç²å–æ¨¡å‹é…ç½®ä¸¦é©—è­‰åƒæ•¸
    console.log('ğŸ”§ ç²å–æ¨¡å‹é…ç½®...')
    const modelConfig = getModelConfigByModelName(provider, model)
    console.log('âœ… æ¨¡å‹é…ç½®:', modelConfig.displayName)

    const validatedParams = validateAndNormalizeParams(modelConfig, {
      ...parameters,
      model // æ·»åŠ æ¨¡å‹åç¨±ä»¥ä¾› transformParams ä½¿ç”¨
    })
    console.log('âœ… é©—è­‰å¾Œçš„åƒæ•¸:', validatedParams)

    // æ ¹æ“šæä¾›å•†è™•ç†è«‹æ±‚
    console.log('ğŸš€ èª¿ç”¨ API...')
    let response: Response

    switch (provider) {
      case 'openai':
      case 'openai-gpt5':
        response = await handleOpenAIRequest(apiKey, messages, model, validatedParams)
        break
      case 'anthropic':
        response = await handleAnthropicRequest(apiKey, messages, model, validatedParams)
        break
      case 'deepseek':
        response = await handleDeepSeekRequest(apiKey, messages, model, validatedParams)
        break
      case 'gemini':
        response = await handleGeminiRequest(apiKey, messages, model, validatedParams)
        break
      case 'novita':
        response = await handleNovitaRequest(apiKey, messages, model, validatedParams)
        break
      default:
        console.error('âŒ æœªçŸ¥çš„æä¾›å•†:', provider)
        return errorResponse(400, "æœªçŸ¥çš„æ¨¡å‹æä¾›å•†")
    }

    console.log('âœ… API èª¿ç”¨æˆåŠŸ')
    return response

  } catch (error: any) {
    console.error('âŒ API èª¿ç”¨å¤±æ•—:', {
      name: error.name,
      message: error.message,
      status: error.status,
      stack: error.stack
    })

    // è™•ç† OpenAI ç‰¹å®šéŒ¯èª¤
    if (error.error) {
      return errorResponse(
        error.status || 500,
        error.error.message || error.message || "API èª¿ç”¨å¤±æ•—"
      )
    }

    return errorResponse(
      error.status || 500,
      error.message || "æ¨¡å‹æœå‹™èª¿ç”¨å¤±æ•—"
    )
  }
}

// Gemini è™•ç†å‡½æ•¸
async function handleGeminiRequest(
  apiKey: string,
  messages: Message[],
  model: string,
  params: Record<string, any>
) {
  const gemini = new GoogleGenAI({ apiKey: apiKey })

  // Get the last message and check its role
  const lastMessage = messages[messages.length - 1]
  const isLastMessageUser = lastMessage.role === 'user'

  // Convert messages to Gemini chat history format
  const historyMessages = isLastMessageUser ? messages.slice(0, -1) : messages
  const history = historyMessages.map(msg => ({
    role: msg.role === 'assistant' ? 'model' : 'user',
    parts: [{ text: msg.content }]
  }))

  const chat = gemini.chats.create({
    model: model,
    history: history,
  })

  // Only send message if last message is from user
  const messageToSend = isLastMessageUser ? lastMessage.content : ''

  const response = await chat.sendMessage({
    message: messageToSend,
    config: {
      maxOutputTokens: params.maxTokens,
      temperature: params.temperature,
    },
  })

  return new Response(
    JSON.stringify({
      role: "assistant",
      content: response.text
    }),
    { headers: { 'Content-Type': 'application/json' } }
  )
}

// OpenAI è™•ç†å‡½æ•¸ï¼ˆåŒ…æ‹¬ GPT-5ï¼‰
async function handleOpenAIRequest(
  apiKey: string,
  messages: Message[],
  model: string,
  params: Record<string, any>
) {
  try {
    console.log('ğŸ¤– OpenAI - åˆå§‹åŒ–å®¢æˆ¶ç«¯')
    const openai = new OpenAI({ apiKey })

    // æ§‹å»ºè«‹æ±‚åƒæ•¸
    const requestParams: any = {
      model,
      messages: messages.map(formatOpenAIMessage),
    }

    // æ·»åŠ æ¨™æº–åƒæ•¸
    if (params.temperature !== undefined) {
      requestParams.temperature = params.temperature
    }

    // æª¢æ¸¬æ˜¯å¦æ˜¯ GPT-5 æ¨¡å‹
    const isGPT5 = isGPT5Model(model)

    // GPT-5 ä½¿ç”¨ max_completion_tokensï¼Œå…¶ä»–æ¨¡å‹ä½¿ç”¨ max_tokens
    if (params.maxTokens !== undefined) {
      if (isGPT5) {
        requestParams.max_completion_tokens = params.maxTokens
      } else {
        requestParams.max_tokens = params.maxTokens
      }
    }

    // æ·»åŠ  GPT-5 å°ˆå±¬åƒæ•¸
    if (isGPT5) {
      console.log('ğŸŒŸ æª¢æ¸¬åˆ° GPT-5 æ¨¡å‹')
      // GPT-5 å°ˆå±¬åƒæ•¸
      if (params.reasoning_effort !== undefined && model !== 'gpt-5-chat-latest') {
        requestParams.reasoning_effort = params.reasoning_effort
        console.log('  âœ“ reasoning_effort:', params.reasoning_effort)
      }
      if (params.verbosity !== undefined && model !== 'gpt-5-chat-latest') {
        requestParams.verbosity = params.verbosity
        console.log('  âœ“ verbosity:', params.verbosity)
      }
    }

    console.log('ğŸ“¤ ç™¼é€è«‹æ±‚åˆ° OpenAI:', {
      model: requestParams.model,
      temperature: requestParams.temperature,
      max_tokens: requestParams.max_tokens || requestParams.max_completion_tokens,
      messageCount: requestParams.messages.length
    })

    const response = await openai.chat.completions.create(requestParams)

    console.log('ğŸ“¥ æ”¶åˆ° OpenAI éŸ¿æ‡‰:', {
      id: response.id,
      model: response.model,
      choices: response.choices?.length,
      usage: response.usage
    })

    if (!response.choices || response.choices.length === 0) {
      throw new Error('OpenAI è¿”å›äº†ç©ºçš„ choices æ•¸çµ„')
    }

    const content = response.choices[0].message.content
    if (!content) {
      console.warn('âš ï¸ OpenAI è¿”å›äº†ç©ºå…§å®¹')
    }

    return new Response(
      JSON.stringify({
        role: response.choices[0].message.role,
        content: content || ''
      }),
      { headers: { 'Content-Type': 'application/json' } }
    )
  } catch (error: any) {
    console.error('âŒ OpenAI API éŒ¯èª¤:', {
      message: error.message,
      type: error.type,
      code: error.code,
      status: error.status
    })
    throw error
  }
}

// DeepSeek è™•ç†å‡½æ•¸
async function handleDeepSeekRequest(
  apiKey: string,
  messages: Message[],
  model: string,
  params: Record<string, any>
) {
  const deepseekClient = axios.create({
    baseURL: "https://api.deepseek.com",
    headers: {
      'Authorization': `Bearer ${apiKey}`,
      'Content-Type': 'application/json',
    },
  })

  const response = await deepseekClient.post('/v1/chat/completions', {
    model,
    messages: messages.map(formatOpenAIMessage),
    temperature: params.temperature,
    max_tokens: params.maxTokens,
  })

  return new Response(
    JSON.stringify({
      role: response.data.choices[0].message.role,
      content: response.data.choices[0].message.content
    }),
    { headers: { 'Content-Type': 'application/json' } }
  )
}

// Novita è™•ç†å‡½æ•¸
async function handleNovitaRequest(
  apiKey: string,
  messages: Message[],
  model: string,
  params: Record<string, any>
) {
  const novitaClient = new OpenAI({
    apiKey,
    baseURL: "https://api.novita.ai/v3/openai"
  })

  const response = await novitaClient.chat.completions.create({
    model,
    messages: messages.map(formatOpenAIMessage),
    temperature: params.temperature,
    max_tokens: params.maxTokens,
  })

  return new Response(
    JSON.stringify({
      role: response.choices[0].message.role,
      content: response.choices[0].message.content
    }),
    { headers: { 'Content-Type': 'application/json' } }
  )
}

// Anthropic è™•ç†å‡½æ•¸
async function handleAnthropicRequest(
  apiKey: string,
  messages: Message[],
  model: string,
  params: Record<string, any>
) {
  const anthropic = new Anthropic({ apiKey })
  const response = await anthropic.messages.create({
    model,
    messages: messages.map(formatClaudeMessage),
    max_tokens: params.maxTokens,
    temperature: params.temperature,
  })

  const content = response.content[0].type === 'text'
    ? response.content[0].text
    : ''

  return new Response(
    JSON.stringify({
      role: response.role,
      content
    }),
    { headers: { 'Content-Type': 'application/json' } }
  )
}

// æ¶ˆæ¯æ ¼å¼åŒ–å‡½æ•¸
function formatOpenAIMessage(message: Message): OpenAI.ChatCompletionMessageParam {
  return {
    role: message.role,
    content: message.content
  }
}

function formatClaudeMessage(message: Message): Anthropic.MessageParam {
  return {
    role: message.role === 'system' ? 'assistant' : message.role,
    content: message.content
  }
}

// æª¢æ¸¬æ˜¯å¦æ˜¯ GPT-5 æ¨¡å‹
function isGPT5Model(model: string): boolean {
  return model.includes('gpt-5') ||
    model === 'gpt-5-preview' ||
    model === 'gpt-5-mini' ||
    model === 'gpt-5-chat-latest'
}

// éŒ¯èª¤éŸ¿æ‡‰å‡½æ•¸
function errorResponse(status: number, message: string) {
  return new Response(
    JSON.stringify({ error: message }),
    { status, headers: { 'Content-Type': 'application/json' } }
  )
}
